# Отчет по учебному исследовательскому проекту - реимплеменация научной статьи "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators"

## Постановка проблемы
>StyleGAN-NADA решает проблему адаптации генераторов изображений к новым доменам без необходимости аннотированных данных.
>StyleCLIP может редактировать изображение только в пределах домена генератора. StyleGAN-NADA  изменяет веса генератора и, следовательно, может выходить за пределы домена. Модель способна применять изменения, которые выходят за рамки всех подходов StyleCLIP.

## Ключевая идея
>Поскольку оптимизирован сам генератор (а не отдельные скрытые векторы), после обучения он может бесконечно производить генерацию изображений в целевом домене.
>Это обеспечивает гибкие zero-shot преобразования, не требуя разметки данных или обучения с нуля.

## Описание решения:
Регуляризация процесса дообучения делается с помощью ограничения количества весов, которые могут меняться во время каждой итерации.
Выбираем k-top релевантных слоев, изменения в которых были теснее всего связаны с выполяемым изменением target -> source.

**Алгоритм выбора слоев для разморозки**
1. Временно размораживаем все слои модели чтобы можно было работать с градиентами.
2. Выбираем слои, которые будем обучать. В нашем случае это convs (используется Style GAN 2 для PyTorch https://github.com/rosinality/stylegan2-pytorch).
3. Проводим итерацию по обучению с помощью global CLIP-loss. 
4. Для обучаемых слоев вычисляем градиент.
5. Выбираем top-k слоев с наибольшей нормой градиента.
6. Замораживаем в convs все слои, кроме выбранных top-k слоев
>select_layers_to_unfreeze # реализция алгоритма

**Как строить обучение**
1. Загружаем модедели и веса:
  - [Installed CLIP](https://github.com/openai/CLIP)
  - [Style GAN 2 (in Pytorch)](https://github.com/rosinality/stylegan2-pytorch)
  - [Weights for StyleGAN2-ffhq One-Shot Adaptation of GAN in Just One CLIP (pytorch)](https://huggingface.co/akhaliq/OneshotCLIP-stylegan2-ffhq/resolve/main/stylegan2-ffhq-config-f.pt)
2. Ищем top-k слоев в convs которые будут разморожены. Остальные слои в convs замораживаем.
3. Для обучения используем directional CLIP-loss.
4. Проводим заранее определенное количество итераций по выбранным промптам. Каждые M (например, 50) итераций выводим пару изображений (текущее и изначальное) для оценки качества сгенерированных изображений.

**Описание эксперимента:**

Гиперпараметры, которые подбирались исходя из рекомендаций в appendix раздел I оригинальной [статьи](https://arxiv.org/pdf/2108.00946):
* Ni = 1 (количество итераций алгоритма по поиску слоев для разморозки);
* Количество слоев, размороженных для обучения - в зависмости от промпта от 3 до 12, иногда 0 - все разморожено;
* lr = 0.002 # рекомендации;

Другие гиперпараметры:
* alpha = 0.5 (коэффициент при вычислении directional CLIP-loss
* latent_dim=512 (размерность латентного пространства)
* image_size = 1024 (выбрана модель stylegan2-ffhq)

Выбирались следующие промпты и параметры размороженных слоев (k-top) и итераций (steps):
1. Shrek from face
k-top: 12 - 10 - 5 - 3
Steps: 300 - 150
best: 3-150

2. Anime paintig from face:
k-top: 10 - 5 - 0, 
Steps: 300, 
best: 10-300.

3. Pixar cartoon from face
k-top: 10, 
Steps: 300, 
best: 10-300.

4. Sketch from face
k-top: 10 - 0 - 8 - 5 - 3 - 12, 
Steps: 300, 
best: 10-300.

5. Zoombie from face
k-top: 12 - 3 - 5 - 8,
Steps: 300,
best: 5-300.

**Описание результатов**

**Выводы**

(что получилось, что не получилось и почему)
